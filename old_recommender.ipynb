{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Recommender Pair Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T07:19:55.650925",
     "start_time": "2017-02-16T07:19:54.501361"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T07:19:55.734524",
     "start_time": "2017-02-16T07:19:55.653056"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T07:20:19.562226",
     "start_time": "2017-02-16T07:20:19.535105"
    }
   },
   "outputs": [],
   "source": [
    "# Build our Spark Session and Context\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark, sc;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading  and Cleaning Data\n",
    "\n",
    "We have two options here\n",
    "\n",
    "1. Load data into a Pandas dataframe, convert to a Spark dataframe\n",
    "    * Careful! This only works because our dataset is small. Usually when we use Spark our datasets are too large to fit in memory.\n",
    "2. Load data into a Spark RDD, convert to a Spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T07:21:51.305220",
     "start_time": "2017-02-16T07:21:51.238519"
    }
   },
   "outputs": [],
   "source": [
    "# Read the ratings data into a Pandas DataFrame\n",
    "ratings_pd_df = pd.read_csv('data/training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T07:20:01.365346",
     "start_time": "2017-02-16T07:19:55.945394"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to a Spark DataFrame, dropping `timestamp` column will happend before the train\n",
    "ratings_df = spark.createDataFrame(ratings_pd_df) #.drop('timestamp', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T07:21:51.703355",
     "start_time": "2017-02-16T07:21:51.670800"
    }
   },
   "outputs": [],
   "source": [
    "# Show Pandas DataFrame\n",
    "# ratings_pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see we built the DataFrame\n",
    "# ratings_df\n",
    "\n",
    "# Take a look at the entries in the DataFrame\n",
    "# ratings_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order dataframe by timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings_df.orderBy('timestamp', ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings_pd_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df    = ratings_df.orderBy('timestamp')\n",
    "ratings_pd_df = ratings_df.toPandas()\n",
    "\n",
    "n_rows = ratings_pd_df.shape[0] # int(n_rows*.8)\n",
    "\n",
    "train = ratings_pd_df.head(int(n_rows*.8))\n",
    "test  = ratings_pd_df.tail(int(n_rows*.2))\n",
    "\n",
    "sp_train = spark.createDataFrame(train.drop('timestamp', axis=1))\n",
    "sp_test  = spark.createDataFrame(test.drop('timestamp', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_pd = pd.read_csv('data/requests.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_req = spark.createDataFrame(req_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "req_pd = pd.read_csv('data/requests.csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T07:20:04.318339",
     "start_time": "2017-02-16T07:20:03.376009"
    }
   },
   "outputs": [],
   "source": [
    "# Check to see our split worked\n",
    "sp_train.count(), sp_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T07:20:04.568199",
     "start_time": "2017-02-16T07:20:04.319761"
    }
   },
   "outputs": [],
   "source": [
    "als_model = ALS(userCol='user',\n",
    "                itemCol='movie',\n",
    "                ratingCol='rating',\n",
    "                nonnegative=True,\n",
    "                regParam=0.01,\n",
    "                maxIter=20,\n",
    "                rank=10,\n",
    "                coldStartStrategy = \"nan\"\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T07:20:12.770205",
     "start_time": "2017-02-16T07:20:12.671744"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommender = als_model.fit(sp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T07:20:12.770205",
     "start_time": "2017-02-16T07:20:12.671744"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions for the whole test set\n",
    "prediction_sp = recommender.transform(sp_req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T07:20:12.770205",
     "start_time": "2017-02-16T07:20:12.671744"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+\n",
      "|user|movie|prediction|\n",
      "+----+-----+----------+\n",
      "|  53|  148|       NaN|\n",
      "|4169|  148| 3.0860252|\n",
      "|5333|  148| 2.2447586|\n",
      "|4387|  148|  2.671124|\n",
      "| 840|  148|       NaN|\n",
      "+----+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_sp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Time to evaluate our model. We'll calculate the RMSE of our predicted ratings and also look at a violin plot of true ratings (x-axis) vs the predicted ratings (y-axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T07:20:18.395843",
     "start_time": "2017-02-16T07:20:12.772391"
    }
   },
   "outputs": [],
   "source": [
    "# Dump the predictions to Pandas DataFrames to make our final calculations easier\n",
    "# predictions_df = predictions.toPandas()\n",
    "# train_df = sp_train.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_pd.shape\n",
    "\n",
    "# predictions_pd.isna().sum()\n",
    "\n",
    "# predictions_pd.isna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_pd = prediction_sp.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.drop('timestamp', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T07:20:19.171884",
     "start_time": "2017-02-16T07:20:18.397394"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill any missing values with the mean rating\n",
    "# There are multiple things you could fill with, this is just one example\n",
    "predictions_pd = predictions_pd.fillna(4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = predictions_pd['rating'] > 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_pd[['rating','prediction']][mask] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_pd['squared_error'] = (predictions_pd['rating'][mask] - predictions_pd['prediction'][mask])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T07:20:19.209038",
     "start_time": "2017-02-16T07:20:19.173337"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_pd['sq_err_45'] = (predictions_pd['rating'][mask] - predictions_pd['prediction'][mask])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T07:20:19.255790",
     "start_time": "2017-02-16T07:20:19.210434"
    }
   },
   "outputs": [],
   "source": [
    "predictions_pd.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T07:20:19.283333",
     "start_time": "2017-02-16T07:20:19.257183"
    }
   },
   "outputs": [],
   "source": [
    "predictions_pd.fillna(0, inplace=True)\n",
    "\n",
    "# Calculate RMSE\n",
    "np.sqrt(sum(predictions_pd['sq_err_45']) / mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_pd.to_csv('data/pred1.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4129060095515595\n",
    "\n",
    "als_model = ALS(nonnegative=True, regParam=0.01, maxIter=20,\n",
    "                rank=10, coldStartStrategy = \"nan\")\n",
    "                \n",
    "2.5177126250676602\n",
    "\n",
    "als_model = ALS(nonnegative=True, regParam=0.01, maxIter=15,\n",
    "                rank=10, coldStartStrategy = \"nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T07:20:19.533590",
     "start_time": "2017-02-16T07:20:19.284926"
    }
   },
   "outputs": [],
   "source": [
    "# Create array of predictions for violinplot\n",
    "data = [predictions_pd['prediction'][predictions_pd['rating'] == rating] for rating in range(1, 6)]\n",
    "\n",
    "plt.violinplot(data, range(1,6), showmeans=True)\n",
    "plt.xlabel('True Ratings')\n",
    "plt.ylabel('Predicted Ratings')\n",
    "plt.title('True vs. ALS Recommender Predicted Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read file into a RDD\n",
    "rdd = sc.textFile('data/u.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see it is loaded\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ugly, looks like we need to clean it up a little\n",
    "# Build a custom function to fix types in the RDD\n",
    "def casting_function(row):\n",
    "    user, movie, rating, timestamp = row\n",
    "    return int(user), int(movie), float(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean up the RDD and drop the timestamp row\n",
    "clean_rdd = rdd.map(lambda row: row.split('\\t')) \\\n",
    "               .map(casting_function) \\\n",
    "               .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a schema for our Spark DataFrame\n",
    "schema = StructType( [ \n",
    "    StructField('user', IntegerType(), True),\n",
    "    StructField('movie', IntegerType(), True),\n",
    "    StructField('rating', FloatType(), True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a Spark DataFrame\n",
    "ratings_df = spark.createDataFrame(clean_rdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T07:20:01.393966",
     "start_time": "2017-02-16T07:20:01.366839"
    }
   },
   "outputs": [],
   "source": [
    "# Check to see we built the DataFrame\n",
    "ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T07:20:03.314810",
     "start_time": "2017-02-16T07:20:01.395377"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the entries in the DataFrame\n",
    "ratings_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T07:20:11.047181",
     "start_time": "2017-02-16T07:20:10.960342"
    }
   },
   "outputs": [],
   "source": [
    "# from our modeling block\n",
    "\n",
    "# # Get the factors for user 1 and movie 100\n",
    "# user_factor_df = recommender.userFactors.filter('id = 4958')\n",
    "# item_factor_df = recommender.itemFactors.filter('id = 1924')\n",
    "\n",
    "# user_factors = user_factor_df.collect()[0]['features']\n",
    "# item_factors = item_factor_df.collect()[0]['features']\n",
    "\n",
    "# # Manually (sorta) calculate the predicted rating\n",
    "# np.dot(user_factors, item_factors)\n",
    "\n",
    "\n",
    "\n",
    "# # Build a single row DataFrame\n",
    "# data = [(4958, 1924)]\n",
    "# columns = ('user', 'movie')\n",
    "# one_row_spark_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# # Check that it worked\n",
    "# one_row_spark_df.show()\n",
    "\n",
    "# # Get the recommender's prediction\n",
    "# recommender.transform(one_row_spark_df).show()\n",
    "\n",
    "# # Let's take a look all the user factors\n",
    "# recommender.userFactors.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
